---
title: "NBA Career Longevity Prediction"
author: "Candidates: 26183, 33349, 34803"
date: "`r format(Sys.time(), '%B %Y')`"
output: rmdformats::readthedown
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)

#library(tidyverse)
library(kableExtra)
library(tree)
#library(broom) # tidymodels
library(caret)
#library(dplyr) # tidymodels
#library(ggplot2) # tidymodels
#library(fastDummies) # pivot_wider 
#library(corrplot) # possibly not doing a corr plot
library(prettydoc)
library(tidymodels)

# LASSO 
library(kernlab)
library(glmnet)
library(gmodels)

# KNN
library(class)

# TREE
library(rpart.plot)

raw_data <- read.csv("data_in/nba_logreg.csv")
raw_data$TARGET_5Yrs <- as.factor(raw_data$TARGET_5Yrs) # treat outcome variable as binary factor

#set.seed(1423) # fix all random outcomes throughout the project
```

# I: Project Introduction

*You can navigate this document by clicking on headings and subheadings in the table of contents on the left*

*Throughout this document, the kable function is used to cleanly display data*

This dataset contains the key performance metrics of NBA players in their rookie (first) season in the NBA League and additional information about how they entered the league (The NBA Draft and the teams they were drafted to).

The objective of this project is to build classification models to predict whether or not players will play at least 5 years in the NBA, given how they performed in their rookie season. We would like to better understand what factors make players more likely to thrive in the NBA. Given the term structure of rookie contracts, players who make it to year 5 have a higher chance of getting another contract and playing several more years.

This dataset was collected from https://data.world/exercises/logistic-regression-exercise-1

Additional data regarding the year players were drafted, what round they were drafted in, the year they debuted, the team they played for and that team's record at the end of the season was web scraped from Wikipedia using a Python script we wrote.

## Variable descriptions

### Outcome
**TARGET_5Yrs**: Dummy variable with value 1 for players who went on to play 5 or more years in the league, and 0 for those that did not

### Predictors
  1) **GP**	- Games played [integer]
  2) **MIN**	- Average minutes played per game [numeric]
  3) **PTS**	- Average points per game	[numeric]	
  4) **FGM**	- Average field goals made per game [numeric]		
  5) **FGA**	- Average field goal attempts	[numeric]	
  6) **FG%**	- Average field goal success percentage [numeric]		
  7) **3P Made** - Average three-pointers made per game [numeric]	
  8) **3PA**	- Average three-pointers attempted per game [numeric]		
  9) **3P%**	- Average three-pointer success percentage per game	[numeric]	
  10) **FTM**	- Average free throws made per game [numeric]		
  11) **FTA**	- Average free throws attempted per game	[numeric]	
  12) **FT%**	- Average free throw success percentage	[numeric]	
  13) **OREB**	- Average offensive rebounds per game [numeric]		
  14) **DREB**	- Average defensive rebounds per game [numeric]		
  15) **REB**	- Average rebounds per game [numeric]				
  16) **AST**	- Average assists per game [numeric]				
  17) **STL**	- Average steals per game [numeric]				
  18) **BLK**	- Average blocks per game [numeric]				
  19) **TOV**	- Average turnovers per game [numeric]				
  20) **Year_Drafted** - Year of draft [integer]		
  21) **Round_Drafted**	- Round of draft [character - contains 'U' for undrafted]		
  22) **Team** - Team played for [character]	
  23) **Win_Percentage** - Team's win percentage [numeric]
  24) **Position** - Position played in [character]		
  25) **Year_Debut** - Year of debut [integer]

## Preview raw data
```{r preview_raw}
sprintf("%s observations of %s variables", nrow(raw_data), ncol(raw_data))
kable(head(raw_data)) # %>%
#  kable_styling(bootstrap_options = c("striped", "bordered", "hover", "condensed", "responsive")) 
# # scroll_box(width = "800px")
#glimpse(raw_data)
#str(raw_data)
```

# II: Exploratory Data Analysis (EDA) of raw data

## Summary statistics 

Using the powerful "summary" function, we can check each column for missing, negative (where there shouldn't be), infinite and anomalous values. We can also make sure all values fall within obvious expected ranges for particular variables (year drafted, win percentage etc.), using the minimum and maximum statistics. 

```{r summary_statistics}
summary(raw_data)
# counts of outcome variable
#kable(raw_data %>% count(TARGET_5Yrs) %>% mutate(proportion = round(n/sum(n),2)))
```

**Insights:**

  1) Values for year drafted and year debut are all in the expected range (between 1971 and 2016)
  2) Values for win percentage are all between 0 and 100, as expected
  3) The character variables (name, round drafted, team and position) all have the expected number of observations (1299).
  4) Using the TARGET_5Yrs variable, we see that 817 (63%) of players in the sample reached 5 years, and 482 (37%) did not
  
5) A small number of NAs are present in the expected 3 pointers column (10 observations). Given this is less than 1% of the sample size, we make the decision to drop all observations with a missing value for any variable, as it will make the model-fitting easier while costing very little in accuracy.

```{r omit_nas}
data_no_nas <- na.omit(raw_data) #%>% select(-Name, -Team, -Year_Drafted)
sprintf("NAs omitted, number of obs falls from %s to %s", nrow(raw_data), nrow(data))
```

## Remove collinear and not-useful variables

For some metrics, such as free throws, we have 3 variables. One for attempts, one for successes, and one for percentage success = 100*(attempts/successes). Including all three variables leads to collinearity when we come to model, as each can be calculated directly from the other two, so we drop the attempts variables, as they intuitively seems least useful. 

Also, total rebounds = offensive rebounds + defensive rebounds, so we drop the total rebounds variable. 

We also drop the following variables:

  1) **Player names**, as they will serve no purpose in our analysis.
  
  2) **Team names**, as most of the predictive power of the team should be captured by the team win percentage variable (which is far easier to work with than 30 dummy variables for the teams).
  
  3) **Year drafted**, as this information is captured more effectively by the year debut variable. Additionally, there are instances where players declare for the draft, but do not make their debut in the same year for various reasons. For example, NBA Hall of Famer David Robinson was selected 1st overall in the NBA Draft by the San Antonio Spurs but could not play for two years because he had to fulfil his active-duty obligations with the Navy.

```{r remove_collinear_and_uninformative}
data <- data_no_nas %>%
  select(-Name, -Year_Drafted, -Team, -FGA, -X3PA, -FTA, -REB)
```

## Check character variable values

Summarise does a good job of making sure the numeric values all fall within the expected ranges. However, non-numeric variables can also contain errors. We check for this by getting the distinct values for the two remaining columns with character type (Round Drafted and Position)

```{r check_character_variable_values}
data %>% count(Round_Drafted) #%>% arrange(Round_Drafted)
data %>% count(Position)# %>% arrange(Position)
```
We find a number of issues:

**Round drafted**

  1) There are two different representations of U (undrafted), because one of them has a trailing space
  
  2) Our dataset doesn't contain any players that were drafted in round 9. This is noted but not likely to cause issues, and is because a) the sample is quite small, and b) round 9 didn't exist past 1989. This will be discussed again later on.
  
  3) Very few players are drafted in other high round numbers, such as round 10 (only 1 player). This may cause problems, because the factor level 10 can only exist in either the training or testing data, but not both.
  
**Position**

  1) Again, there are two different representations of Center, due to a trailing space
  
  2) There are two different representations of Point guard, this time due to a letter case
  
We now fix these issues:

```{r fix_character_variable_values}
# remove any leading or trailing whitespace in character columns
data <- data.frame(lapply(data, function(x) if(class(x)=="character") trimws(x) else(x)), stringsAsFactors=F)

# replace 'Point Guard' with 'Point guard'
data$Position[data$Position == "Point Guard"] <- "Point guard"

# check it worked
data %>% count(Round_Drafted) #%>% arrange(Round_Drafted)
data %>% count(Position) #%>% arrange(Position)
```

## Correlations of regressors with outcome variable

As a simple way to get a sense of which of the numeric regressors are most likely to have predictive power, we take the correlation coefficient of each numeric regressor with the outcome variable (TARGET_5Yrs)

```{r corr_regressors_with_outcome}
# generate correlation matrix 
correlations <- round(cor(as.numeric(data$TARGET_5Yrs), Filter(is.numeric, data)),2)
correlations[,order(abs(correlations),decreasing = TRUE)] # display, ordered by absolute values
```

While the correlation coefficient only measures linear relationships, we conclude that metrics involving three-pointers, and years of draft and debut, are likely to be poor predictors of success. Interestingly, so too is team win percentage - suggesting players cannot simply ride the coattails of their team to achieve individual success.

As we have a large number of predictors, this information may be used when deciding which predictors to include in certain models. 

<!-- ```{r corrs_between_regressors} -->
<!-- library(plotly) -->

<!-- corr_matrix = cor(Filter(is.numeric, raw_data)) -->
<!-- corr_matrix_diagonal = corr_matrix -->
<!-- corr_matrix_diagonal[lower.tri(corr_matrix_diagonal, diag = TRUE)] <- NA -->
<!-- corr_matrix_diagonal <- corr_matrix_diagonal[-1, -ncol(corr_matrix_diagonal)] -->
<!-- #corr_matrix = cor(raw_data %>%  select(TOV, BLK)) -->
<!-- corrplot(corr_matrix , type = "upper", tl.col = "black" , tl.srt = 45) -->

<!-- plot_ly(x = rownames(corr_matrix), y = colnames(corr_matrix), z = corr_matrix_diagonal, type = "heatmap") -->

<!-- ``` -->

<!-- ## Distributions of numeric variables, by outcome -->

<!-- ```{r, fig.height= 10, fig.width = 10, warning=FALSE} -->
<!-- numerics_long <- Filter(is.numeric, data) %>%  -->
<!--   pivot_longer(!TARGET_5Yrs, names_to = "variable", values_to = "value") -->
<!-- numerics_long$TARGET_5Yrs <- as.character(numerics_long$TARGET_5Yrs) -->

<!-- ggplot(numerics_long, aes(value, fill = TARGET_5Yrs)) + geom_boxplot() + facet_wrap(~variable, scales = "free") + labs(x = "") -->
<!-- # ggplot(numerics_long, aes(value, fill = TARGET_5Yrs)) + geom_histogram() + facet_wrap(~variable, scales = "free") + labs(x = "") -->
<!-- ``` -->

<!-- We see that for most variables, the distribution of values for players who went on to succeed is higher. This makes sense, as most of these variables are increasing in player quality (the better player is expected to make more blocks, steals etc.)  -->

## Distributions of character variables, by outcome

For the non-numeric variables, we can't use correlation coefficients to gauge predictive power. However, we can inspect the relationship between the variable and the outcome through bar charts.

```{r char_vars_bar_charts, warning=FALSE}
# plot for round drafted
data %>%
  select(Round_Drafted, TARGET_5Yrs) %>%
  group_by(Round_Drafted, TARGET_5Yrs) %>% tally() %>%
  group_by(TARGET_5Yrs) %>% mutate(proportion = n/sum(n)) %>%
  ggplot(aes(x = Round_Drafted, y = proportion, fill = TARGET_5Yrs)) + geom_bar(stat = "identity", position = position_dodge()) + scale_x_discrete(limits = c(seq(1, 10), "U")) + ylab("proportion of players drafted in the round, by outcome")

# plot for position
data %>%
  select(Position, TARGET_5Yrs) %>%
  group_by(Position, TARGET_5Yrs) %>% tally() %>%
  group_by(TARGET_5Yrs) %>% mutate(proportion = n/sum(n)) %>%
  ggplot(aes(x = Position, y = proportion, fill = TARGET_5Yrs)) + geom_bar(stat = "identity", position = position_dodge()) + scale_x_discrete(limits = c("Point guard", "Shooting guard", "Center", "Power forward", "Small forward")) + ylab("proportion of players drafted in that round") + ylab("proportion of players in the position, by outcome")
```

**Round drafted**

In basketball, better players are generally drafted in an earlier round (with a lower round number), with the undrafted (U) players likely to be the worst. 

There is a clear difference in the distribution of round numbers for those who reached 5 years and those who did not. Among those with TARGET_5Yrs = 1, more than 65% were drafted in the first round, and fewer than 10% were undrafted. Among those who didn't reach 5 years, the corresponding values are 30% and 20%.

**Position**

To give the order of the bars some meaning, we order by position on the field (i.e. point guards are furthest back, and small forwards furthest forward).

We are then able to see that not only is there a slight difference in distribution of positions between those who made 5 years and those who didn't, but also the difference is successful players are slightly skewed towards attacking positions.

We conclude that both these variables are likely to be of value. 

# III: Pre-process dataset

We now move on to first preparing the data for, and then fitting, models. Where possible, we will do this using tidymodels methods, to ensure all models use the same data for training and testing, and to make it easier to fit and compare several models.

## Create indicator variables

Using the "step_dummy" function from recipes (part of tidymodels), create dummy columns for the categoric variables (Round Drafted and Position). The function replaces the categoric variable by n-1 dummy columns, where n is the number of categories the variable can take, to avoid the dummy variable trap.

```{r create_indicators, warning=FALSE}
# step_dummy doesn't give the first value of the variable a dummy. For round drafted, it makes sense for this to be undrafted players, so we code U as 0 so it becomes the first value
data$Round_Drafted[data$Round_Drafted == "U"] <- 0

# define recipe - create dummies for round drafted and position
dummy_multi_choice_rec <- recipe(~ ., data = data) %>%
  step_dummy(Round_Drafted, Position) %>%
  prep()

# execute the steps in the recipe
data_to_split <- bake(dummy_multi_choice_rec, new_data = data) #%>% select(-Name)
#ids <- tidy(dummy_multi_choice_rec, number = 1)

kable(head(data_to_split))
# glimpse(data_to_split) #
```


## Correct the Round Drafted variable

In 1989 the structure of the draft changed from 10 rounds to just 2. So, post-1989, fewer players were drafted each year, and rounds drafted values other than 1, 2 and undrafted ceased to exist.

To correct for this change in meaning of the draft, we will need to create a time dummy variable and add interaction terms to the Round_Drafted_# dummy variables. 


```{r round_drafted_year_debut_interactions}
# Create Dummy variables for debut before 1989
data_to_split$debut_before_1989 <- ifelse(data_to_split$Year_Debut >= 1989, 0, 1)
#data_testing_lasso$debut_before_1989 <- ifelse(data_testing_lasso$Year_Debut >= 1989, 0, 1)

# Vector of names for the new interaction columns
names <- paste0("Round_Drafted_", c("1","10","2","3","4","5","6","7","8"), "_before_1989")

# Create interaction columns for Round_Drafted_# with Year_debut_before_1989
data_to_split[names] <- lapply(select(data_to_split,starts_with("Round_Drafted")), function(x) x*data_to_split$debut_before_1989)
# data_testing_lasso[names] <- lapply(select(data_testing_lasso,starts_with("Round_Drafted")), function(x) x*data_testing_lasso$debut_before_1989)
```


## Split dataset into training and testing

Using the "initial_split" function from rsample (part of tidymodels), split data into training and testing, using (the default) 75% of observations for training and 25% for testing. The same data and samples will be used by all models. To preserve the overall class distribution, we stratify on the outcome variable (TARGET_5Yrs).

```{r train_test_split}
set.seed(1423) # fix training-testing split for our project
data_split <- initial_split(data_to_split, strata = TARGET_5Yrs)

data_training <- data_split %>% training()
data_testing <- data_split %>% testing()

sprintf("Data had %s obs, now split into training (%s obs) and testing (%s obs)", nrow(data), nrow(data_training), nrow(data_testing))

# check proportions of outcome variable values are the same across training and testing
kable(data_testing %>% count(TARGET_5Yrs) %>% mutate(proportion = round(n/sum(n),2)))
kable(data_training %>% count(TARGET_5Yrs) %>% mutate(proportion = round(n/sum(n),2)))

# set.seed(1450)
# ## undummied
# data_split_undummied <- initial_split(data, strata = TARGET_5Yrs)
# 
# data_training_undummied <- data_split_undummied %>% training()
# data_testing_undummied <- data_split_undummied %>% testing()
```

# IV: Model development

## Logistic regression

#### A baseline: logistic regression with all predictors

The model development section starts with a logistic regression model fit on all predictors, using tidymodels. As an introductory stage, the model will also be used to evaluate significance of our predictor variables according to the t-test statistics in the summary below.

```{r logistic_regression_model, warning=FALSE}
logistic_model <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification") %>%
  fit(TARGET_5Yrs ~., data=data_training)
kable(tidy(logistic_model) %>% arrange(p.value)) %>% scroll_box(height = "400px") #%>% as_tibble() %>% print(n=40)
```

*Note*: The logistic model includes one dummy variable for all but one level (the reference level) of each categorical variable.

We can use the p-values or t-test statistics shown in the output to conclude whether the variable coefficients are significant at the 5% level. We are interested in whether the coefficients are statistically different from 0. In other words, we are interested in coefficients with p-values are below 0.05 or equivalently with t-test statistic values above 1.9600. We see that the coefficients statistically significant at the 5% significance level are:

* GP: games played per season
* Win_Perc: the winning percentage of the team the player plays for
* Year_Debut: the year the player debuted his career
* (Intercept):
* Round_Drafted_X1: Indicator Variable for players drafted in Round 1 
* FG.: Average field goal success percentage
* X3P.Made: Average number of Three-Pointers made per game
* OREB: Average number of offensive rebounds per game

This is indeed a striking result: one might expect the game performance of a player to be indicative of his talent and therefore of his ability to go further in the NBA. With the debut and draft year variables being significant, there seems to be a time trend in the NBA drafting process whereby talented players tend to get drafted around the same time. A possible explanation is that drafting more talented players at the same time increases competitiveness in the league, pushing talented players to play even better and stay longer in the NBA.

However, we should note that all the variables related to points scored by a player in a game such as the total number of points, free throws, 3 pointers etc. are all highly correlated. In particular, this may mean that the insignificance of the corresponding coefficients is a result of multicollinearity and not the true lack of an association with the outcome variable.
Similarly, it may be argued that performance on offense and defense captured by variables such as steal, block, assist, rebound etc. exhibit correlation. Top players in the NBA are generally good in both offense and defense.

A final note on significance regarding categorical variables:
Logistic regression uses *reference level coding*. The z-test compares each of the non-reference categories to the reference (omitted) one. It is quite possible for none of those to be significant, but there to be significant differences amongst the non-reference categories.

**Predictions and model accuracy**

```{r logistic_evaluation, warning = FALSE}
# predictions
logistic_pred_results <- logistic_model %>%
  predict(data_testing) %>%
  bind_cols(data_testing) #%>%
#  glimpse()
# logistic_pred_probs <- logistic_model %>%
#   predict(data_testing, type="prob") %>%
#   bind_cols(data_testing)

#accuracy
#logistic_pred_results$TARGET_5Yrs <- as.factor(logistic_pred_results$TARGET_5Yrs)
##truth refers to the actual results and estimate refers to what the model predicted
kable(metrics(logistic_pred_results, truth=TARGET_5Yrs, estimate = .pred_class))
```

The logistic regression model yields an accuracy of nearly 79%. For our classification task, this can be seen as a good score indicating that a linear classification boundary fits our data well.

### Implementation of Gradient Descent

As a first step towards fitting a logistic regression model, we implement a gradient descent algorithm to find the model parameters that minimise the log loss function given by:

$$
L(\beta) = - \frac{1}{n}\sum_{i=1}^{n} [y_{i} \times log(\sigma(x_{i}^T \beta)) + (1-y_{i})\times log(1-\sigma(x_{i}^T \beta))]
$$

where:

* X is the matrix of  predictor variables
* y is the vector of the outcome variable
* $\beta$ is the vector of coefficients in the logistic regression model
* $\sigma$ is the logistic sigmoid function given by $\sigma(x_i^T\beta) = \frac{1}{1+e^-(x_i^T\beta)}$

For simplicity we omit the division by 1/n as it doesn't affect our minimisation problem.

**Note**: Here we assume an unregularised logistic regression with 0 penalty term

```{r sigmoid_loss_function}
# defining the sigmoid function
sigmoid <- function(x) {
  1 / (1 + exp(-x))
}

# implementing the loss function in code
logistic_loss <- function(X, y, beta) {
  X <- unname(as.matrix(X)) # remove column names
  y <- as.numeric(as.character(y)) # converting the factors of the outcome first to strings then to numeric type
  loss <- 0
  for (i in 1:nrow(X)) {
    X_i_B <- sum(X[i, ] * beta)
    p_hat <- sigmoid(X_i_B)
    loss <- loss + (y[i] * log(p_hat) + (1 - y[i]) * log(1 - p_hat))
  }
  loss
}
```

The gradient of the log loss function is:

$$
\nabla L(\beta) = X^T(\sigma(X\beta)-Y)
$$

```{r gradient_function}
# implementing the gradient in code
gradient <- function(X, y, beta) {
  X <- unname(as.matrix(X))
  y <- as.numeric(as.character(y))
  beta <- as.matrix(beta, length(beta), 1) # converting a beta to a column vector of length length(beta)
  as.matrix(t(X) %*% (sigmoid(X %*% beta) - y), ncol(X), 1) # returning the gradient vector as a column vector of length ncol(X)
}

# the gradient descent algorithm
# source: https://en.wikipedia.org/wiki/Gradient_descent#Description
gradient_descent <- function(X, y, loss_function) {
  X <- unname(as.matrix(X))
  X <- scale(X) # standardising the predictors (columns in the X matrix) for greater stability
  y <- as.numeric(unlist(y))
  i <- 0
  beta_prev <- as.matrix(rep(0, ncol(X)), ncol(X), 1) # initialise the coefficient vector to be all zero
  initial_loss_value <- logistic_loss(X, y, beta_prev)
  gamma <- 0.01 # the step
  beta_next <- beta_prev - as.matrix((gamma * gradient(X, y, beta_prev)), ncol(X), 1)
  while (abs(loss_function(X, y, beta_next) - loss_function(X, y, beta_prev)) > 0.001) {
    beta_diff <- gradient(X, y, beta_next) - gradient(X, y, beta_prev)
    gamma_num <- as.numeric(t(beta_next - beta_prev) %*% beta_diff)
    gamma_denom <- sum(beta_diff^2)
    gamma <- abs(gamma_num) / gamma_denom # Update step
    beta_prev <- beta_next
    beta_next <- beta_prev - as.matrix((gamma * gradient(X, y, beta_prev)), ncol(X), 1)
    i <- i + 1
  }
  final_loss_value <- logistic_loss(X, y, beta_next)
  print(paste0("Initial loss: ", initial_loss_value))
  print(paste0("Final loss after ", i, " iterations: ", final_loss_value))
#  print(beta_next) # estimated coefficient values
  return(beta_next)
}
```

We now apply the gradient-descent algorithm to estimate the coefficients of a logistic regression of our target on a subset of predictors.

{The chosen predictors are likely to be weakly correlated, hence they avoid a potential multicollinearity problem in our model.
The chosen predictors (which are continuous variables) capture the general performance of a player in both offence and defence without going into more sophisticated details.
The predictors are: games played, points, rebounds?, assists, steals, blocks and turnovers.}

We use the predictors with p-values below 0.05 in the baseline logistic regression.

```{r apply_gradient_descent}
regressors_grad_desc <- c("GP", "Win_Perc", "Year_Debut", "Round_Drafted_X1", "FG.", "X3P.Made", "OREB")

data_grad_desc <- data_training %>% select(c(all_of(regressors_grad_desc),TARGET_5Yrs))

# scaling the regressors
for (var in regressors_grad_desc){
  data_grad_desc[ , var] <- scale(data_grad_desc[ , var])
}
X_gd <- data_grad_desc %>% select(-TARGET_5Yrs)
y_gd <- data_grad_desc %>% select(TARGET_5Yrs)
coefficients_gd <- gradient_descent(X_gd, y_gd, logistic_loss)
```

To evaluate the efficiency of our algorithm, we compare the estimated coefficients from the gradient descent algorithm to those from the direct implementation of the logistic regression (using the glm library)

```{r direct_logistic_regression_glm}
logistic_fit <- glm(unlist(y_gd) ~ GP + Win_Perc + Year_Debut + Round_Drafted_X1 + FG. + X3P.Made + OREB - 1, data=data_grad_desc, family="binomial") #we omit the intercept
#summary(logistic_fit)

# logistic_fit$coefficients
# as.double(coefficients_gd)

# display logistic fit and gradient descent coefficients side-by-side
kable(list(logistic_fit$coefficients, as.double(coefficients_gd)))
```

As we can see, the estimated coefficients are quite similar between the two implementations (direct implementation of logistic regression, right, and gradient descent, left), confirming that the gradient descent algorithm is a highly efficient optimisation algorithm.

## LASSO + KNN
  <!-- - Relatively interpretable  -->
  <!-- - High Dimensionality  -->

With 40 different regressors, before we fit a KNN model, we want to complete some process of subset selection to not only draw some inference about which variables may be more significant, but also reduce the dimensionality of the KNN model as KNN is susceptible to the curse of dimensionality. For this, we use LASSO.

When considering the effect of Round_Drafted:

\begin{equation}

\beta_1Round\_Drafted\_\# + \beta_2Round\_Drafted\_\# \  * Year\_Debuted\_Before\_1989 \\

(\beta_1 \ +   \beta_2Year\_Debuted\_Before\_1989 )Round\_Drafted\_\# 

\end{equation}

Where:  

\begin{equation}   
Year\_Debuted\_Before\_1989 = 
     \begin{cases}
       1 &\quad\text{if Year_Debut}<1989\\
       0 &\quad\text{if Year_Debut} \ge1989 \\
     \end{cases}
\end{equation}    

Below we are going to estimate the Lasso Regression model using the GLMNET function. This entails setting alpha = 1. It should be noted that by changing the family parameter to "binomial", the GLMNET function "fits a traditional logistic regression model for the log-odds" according to the documentation. For Lambda, we are using a sequence from -6 to 0 in increments of 0.5. 

```{r lasso_model}
data_training_lasso <- data_training %>% select(-TARGET_5Yrs)
data_testing_lasso <- data_testing %>% select(-TARGET_5Yrs)

outcome_training <- data_training %>% select(TARGET_5Yrs)
outcome_testing <- data_testing %>% select(TARGET_5Yrs)

# Estimate the Lasso Regression Model 
model_lasso <- glmnet(data.matrix(data_training_lasso), y = data.matrix(outcome_training), intercept = FALSE, alpha = 1 , lambda = 10^seq(from = -6, to = 0, by = 0.5), family = "binomial")
plot(model_lasso, xvar = "lambda") #Plot the Lasso Regression Model against Log Lambda Values
beta_hat <- coef(model_lasso)
# #Return the coefficients of the Lasso Regression Model for the different values of Lambda 
#beta_hat

# Optimal value of Lambda: estimates the value for Lambda that gives the largest area beneath the ROC (Receiver Operating Characteristic) Curve. These areas are estimated using K-fold cross-validation. 
cv_lasso <- cv.glmnet(x = data.matrix(data_training_lasso), y = data.matrix(outcome_training), type.measure = "mse" , nfolds = 10)
lambda = cv_lasso$lambda.min
#Return the coefficients associated with the model that produces the largest area under the ROC curve in the training data. 
beta_hat2 <- coef(model_lasso, s = lambda)
plot(cv_lasso)
#print(beta_hat2)
beta_hat2[which(rowSums(beta_hat2) > 0),] # print non-zero coefficients
```

### Analysis of the Lasso model above 
**Significant predictors**

Logistic regression suggests: GP, Win_Perc, Year_Debut, Round_Drafted_X1, FG., X3P.Made, OREB
LASSO suggests: GP, MIN, PTS, FG., FTM, OREB, Win_Perc, Round_Drafted_X1, Round_Drafted_1_before_1989


Printing the coefficient estimates for a range of Lambdas was insightful as it was interesting to see what variables persist as the penalty increases. 

Most Notably: 

  -GP (Number of Games Played in Debut Season) 
  
  -MIN (Average Number of Minutes played per game) 
  
  -PTS (Average Number of Points per Game) 
  
  -FG. (Field Goal Percentage)  
  
  -OREB (Average Offensive Rebounds per Game)
  
  -Win_Perc (Win Percentage of Team in Debut Season) 
  -Year_Drafted (Year Drafted), 
  -Round_Drafted_1 (Dummy Variable for Players Drafted in Round 1) 
  -Round_Drafted_1_before_1989 (Interaction Variable for Players Drafted in Round 1 Before 1989)


One could develop a convincing story to justify the coefficients displayed above: 

Teams that win more games with a particular group will be more likely to keep most of those players, especially the main contributors. Therefore, we should not be surprised that the coefficients for Win Percentage and Number of Games Played are positive and relatively large. 

What perhaps is most interesting is the fact that Defensive Rebounds, Steals and Turnovers seem to have a relatively insignificant impact on a player's ability to reach year 5. Whilst there are players who pride themselves on their ability to defend (E.g. Draymond Green - 4x NBA All-Defensive First Team), in reality the structure of the game seems to lend itself more to offensive attributes and point scoring ability.  


It is important to note that we cannot consider this regression causal. Our investigation does not revolve around some treatment variable that we could conclude is randomly assigned. Additionally, there are still confounding variables that are not accounted for in our dataset.  

For example, we have no information on the injury status of players. Players may have played through injuries or been injured in their final college season, thus influencing what round they get drafted in.

Several players like Blake Griffin, Joel Embiid, Ben Simmons and Julius Randle suffered season ending injuries in their rookie seasons. Despite this, all of these plays have gone on to become NBA stars and play beyond 5 years. 
  
Our current specification fails to accurately capture circumstances where confounding variables like injury and health, team chemistry, coaching, and off-the-field status can impact the likelihood of a player reaching year 5. 
  
The data suggests an association between being drafted in the first round and the likelihood of making it to year 5. This is consistent with the notion that players drafted in the first round will have performed better in college or international leagues before declaring for the draft. 

One could argue that the round a player is drafted in functions as a proxy for what teams expect their ability to be. It must be noted that some teams may not certain draft players for schematic or logistical reasons; Some teams may need to improve a particular position and not draft the best available player. 

However, one could therefore argue that teams may be more lenient with first round draft picks and give them more opportunities than lower drafted players; Teams believe that first round draft picks have a higher ceiling and may just need time to adapt to the NBA. 


Using the results from the Lasso Regression, we want to choose the following subset of regressors to train our KNN model: GP, MIN, PTS ,FG., FTM, OREB, Win_Perc, Year_Debut, Round_Drafted_X1, Round_Drafted_1_before_1989

As KNN models are susceptible to the curse of dimensionality, it is important to choose the smallest number of variables possible whilst trying to maximise the amount of information gained.

Looking at the Lasso results above, this subset of regressors appears to represent the variables that capture the most information. 


```{r knn}
vars_lasso_output <- c("GP", "MIN", "PTS", "FG.", "FTM", "OREB", "Win_Perc", "Round_Drafted_X1", "Round_Drafted_1_before_1989")

data_training_lasso_output <- data_training %>% select(c(all_of(vars_lasso_output),TARGET_5Yrs))
data_testing_lasso_output <- data_testing %>% select(c(all_of(vars_lasso_output),TARGET_5Yrs))

# use sqrt(training obs count) for k, as widely suggested 
knn <- nearest_neighbor(mode = "classification", engine = "kknn",
                        neighbors = round(sqrt(nrow(data_training_lasso_output)))) %>%
  fit(TARGET_5Yrs ~., data = data_training_lasso_output)

predictions <- knn %>%
  predict(data_testing_lasso_output) %>%
  bind_cols(data_testing_lasso_output)
#predictions$TARGET_5Yrs <- as.factor(predictions$TARGET_5Yrs)
kable(metrics(predictions, truth=TARGET_5Yrs, estimate = .pred_class))
```

KNN yields an accuracy of nearly 77%, which is quite high considering it is a simple model. This suggests the LASSO variable selection worked well.

## SVM
#### High-dimensional, with a focus on predictive accuracy

As a next step, we check for non-linearity using another classification approach - that of Support Vector Machines (SVM). SVM allows us to classify data points according to non-linear decision boundaries.

### SVM with a radial kernel

When using a Radial Basis (RBF) Kernel, the SVM algorithm measures the Euclidian distance (as a measure of similarity) between points in the space. Consequently, it has a very local behaviour in the sense that only nearby training observations affect the classification of a test observation.
Similar to the KNN algorithm, RBF Kernel has the advantage of overcoming space complexity as, instead of reusing all of the dataset, it only needs to store the support vectors during training. The support vectors are the data points that are the closest to the decision boundary and define the margin of the classifier

```{r svm}
svm_rbf <- svm_rbf() %>%
  set_engine("kernlab") %>%
  set_mode("classification") %>%
  fit(TARGET_5Yrs ~ ., data = data_training)
#svm_rbf

## Predictions and model accuracy
predictions_svm <- svm_rbf %>%
  predict(data_testing) %>%
  bind_cols(data_testing)
#predictions_svm$TARGET_5Yrs <- as.factor(predictions_svm$TARGET_5Yrs)
kable(metrics(predictions_svm, truth=TARGET_5Yrs, estimate = .pred_class))
```

The RBF Kernel yields a predictive accuracy of nearly 77%.

### SVM with a polynomial kernel

```{r svm_polynomial_kernel}
svm_poly <- svm_poly() %>%
  set_engine("kernlab") %>%
  set_mode("classification") %>% 
  fit(TARGET_5Yrs ~., data=data_training)
#svm_poly

## Predictions and model accuracy
predictions_svm_poly <- svm_poly %>%
  predict(data_testing) %>%
  bind_cols(data_testing)
#predictions_svm_poly$TARGET_5Yrs <- as.factor(predictions_svm_poly$TARGET_5Yrs)
kable(metrics(predictions_svm_poly, truth=TARGET_5Yrs, estimate = .pred_class))
```

The polynomial kernel yields an accuracy of nearly 79%.

## Decision Tree
### An interpretable non-baseline model

We now move onto a very different classification approach: decision-tree classification. Decision trees have the advantage of being very interpretable, allowing us to directly visualise the relationship between the outcome and the explanatory variables.

As decision trees are very susceptible to the curse of dimensionality, we will not use the full set of regressors, and use the subset identified as significant by the earlier LASSO method. 

```{r decision_tree}
#data_training$TARGET_5Yrs <- as.factor(Train$TARGET_5Yrs)
tree <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification") %>%
  fit(TARGET_5Yrs ~ ., data = data_training_lasso_output)

tree %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE, type = 5)

##Predictions and model accuracy
predictions_tree <- tree %>%
  predict(data_testing_lasso_output) %>%
  bind_cols(data_testing_lasso_output)
#predictions_tree$TARGET_5Yrs <- as.factor(predictions_tree$TARGET_5Yrs)
kable(metrics(predictions_tree, truth=TARGET_5Yrs, estimate = .pred_class))
```

**Interpretation of results**

 * Year of debut seems to be the most important factor determining the success of an NBA player.
  * Time as a factor: Regardless of other factors, it seems that players who debuted after 2013 do not have long careers in the NBA. This may be due to the fact that the 2011-2012 season was shortened from the normal 82 games per team to 66, after a new collective bargaining agreement (CBA) took place between owners of NBA teams and the players themselves, causing nearly two months of inactivity before it has been finally reached. Although such lockouts are not common in the NBA league, capital loss and concurrency between small and big market teams can cause imbalance within the NBA Association. Such lockouts are expected to decrease the performance of players as well as the potential of talented rookies getting into the league in the course of the season.
 * The round the player gets drafted in is a second important factor. It seems that players who do not get drafted in the 1st round are almost guaranteed to last long in the NBA (provided they play more than 50 games per season). Although this result may seem surprising, it is usually the case that team owners prefer drafting players in subsequent rounds rather than in the first, given how much expectations are put in the first rounders and how much it is expensive to hire them.

* This tree-like visualisation is very useful in that it allows one with very little background understanding to generate predictions regarding the classification of an NBA player, by moving through the tree from top to bottom.
  * Every node consists of 3 numbers: the top one denotes the predicted class (0 or 1), the middle one denotes the probability of belonging to class 1, and last one denotes the percentage of the observations used in that node.

**Observations**

* The tree classifier yields an overall predictive accuracy of 74%.
* The relatively low predictive accuracy is compensated by the simplicity and the interpretability of the decision tree model. Since there are several other factors affecting a player's success in the NBA, some of which we can't control for in our models (such as talent, socio-economic background etc.), it is not surprising that our decision tree does not achieve an exceptionally high predictive accuracy. For what it is, the model seems to be performing reasonably well.

## Random Forests

To improve upon the performance of the decision tree classifier, we consider a random forest model. Random forests use decision trees as building blocks to construct more powerful predictive models.

Decision trees have low bias (they make no assumption about the functional form of the true relationship between the outcome and the predictors) but suffer from high variance (in that the structure of the decision tree is highly sensitive to changes in individual points of the data, so the model certainly overfits). Random forests attempt to address that issue by lowering variance at the expense of a small increase in bias.

Random forest is built by fitting multiple decision trees, then taking the average of the resulting predictions. Each time a split is considered, a random sample of m predictors is chosen as split candidates from the full set of predictors. This ensures that the resulting predictions of the decision trees are not highly correlated (averaging over highly correlated values does not lead to substantial reduction in variance).

As random forests can deal with high-dimensional data, we will use all regressors (excluding those identified as collinear or irrelevant).
```{r random_forest}
set.seed(1423) # set seed to fix outcome of random forest

#random forests fit with tidymodels
rf <- rand_forest(trees = 100, mode="classification") %>%
    set_engine("randomForest") %>%
    fit(TARGET_5Yrs~., data = data_training)
#rf
## Predictions and model accuracy
predictions_rf <- rf %>%
  predict(data_testing) %>%
  bind_cols(data_testing) #%>%
#  glimpse()
#using metrics to measure the performance of the model
#predictions_rf$TARGET_5Yrs <- as.factor(predictions_rf$TARGET_5Yrs)
#truth refers to the actual results and estimate refers to what the model predicted
kable(metrics(predictions_rf, truth=TARGET_5Yrs, estimate = .pred_class))
```

Random forests achieve a predictive accuracy of nearly 79% which is higher than our previous single decision tree classifier, as expected.
```{r rf_probabilities_roc}
#obtaining the probabilities for each possible predicted value
predictions_probs <- rf %>%
  predict(data_testing, type="prob") %>%
  bind_cols(data_testing)
#glimpse(predictions_probs)

#plotting the results
predictions_probs %>%
  roc_curve(TARGET_5Yrs, .pred_0) %>%
  autoplot()
```

The Receiver Operating Characteristic (ROC) curve plots sensitivity (true positive rate TPR) against 1-specificity (false positive rate FPR). It is known to be a good measure of the performance of a binary classifier.

 * *Specificity*: measure of a model's ability to predict true negatives. It measures the proportion of observations that were correctly predicted to be positive out of all negative observations.
 
 * *Sensitivity*: measure of a model's ability to predict true positives. It measures the proportion of observations that were correctly predicted to be positive out of all positive observations.

*Note*: The ROC measure is useful as it does not depend on the class distribution.

 Our random forest classifier yields a ROC curve that is significantly above and far of the baseline 45 degrees line where FPR = TPR. This means our classifier gives fairly accurate results in accordance with its relatively high predictive accuracy.





<!-- # ```{r KNN_variable_selection} -->
<!-- # Adj_Train_data_KNN <- data_training %>% select("Win_Perc","GP", "MIN", "PTS", "FG.", "OREB", "AST", "Year_Drafted", "Round_Drafted_X1") -->
<!-- # Adj_Test_data_KNN <- data_testing %>% select("Win_Perc","GP", "MIN", "PTS", "FG.", "OREB", "AST", "Year_Drafted", "Round_Drafted_X1") -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # KNN_Predicted <- knn(train = data.matrix(Adj_Train_data_KNN), test = data.matrix(Adj_Test_data_KNN), cl = data.matrix(outcome_training), k = 25) -->
<!-- # CrossTable(x= data.matrix(outcome_testing), y= KNN_Predicted, prop.chisq = FALSE, chisq = TRUE) -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # (62+176)/324*100 -->
<!-- # ``` -->
<!-- #  -->
<!-- # accuracy on test data = 73.45679% -->

# V: Conclusion