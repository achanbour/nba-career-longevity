---
title: "NBA Career Longevity Prediction"
author: "Candidates: 26183, 33349, 34803"
date: "`r format(Sys.time(), '%B %Y')`"
output: rmdformats::readthedown
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)

#library(tidyverse)
library(kableExtra)
library(tree)
#library(broom) # tidymodels
library(caret)
#library(dplyr) # tidymodels
#library(ggplot2) # tidymodels
#library(fastDummies) # pivot_wider 
library(corrplot) # possibly not doing a corr plot
library(prettydoc)
library(tidymodels)

# LASSO 
library(kernlab)
library(glmnet)
library(gmodels)

# KNN
library(class)

# TREE
library(rpart.plot)

raw_data <- read.csv("data_in/nba_logreg.csv")
raw_data$TARGET_5Yrs <- as.factor(raw_data$TARGET_5Yrs)
```

# I: Project Introduction

This dataset contains the key performance metrics of NBA players in their rookie (first) season in the NBA League and additional information about how they entered the league (The NBA Draft and the teams they were drafted to).

The objective of this project is to build classification models to predict whether or not players will play at least 5 years in the NBA, given how they performed in their rookie season. Given the term structure of rookie contracts, players who make it to year 5 have a higher chance of getting another contract and playing several more years.

This dataset was collected from https://data.world/exercises/logistic-regression-exercise-1

Additional data regarding the year players were drafted, what round they were drafted in, the year they debuted, the team they played for and that team's record at the end of the season was web scraped from Wikipedia using a Python script not included in this document.

Before being web scraped, the dataset was checked for repeated values and missing values, typos and errors. The data was validated using https://www.basketball-reference.com/ ???

## Variable descriptions

### Outcome
**TARGET_5Yrs**: Dummy variable with value 1 for players who went on to play 5 or more years in the league, and 0 for those that did not

### Predictors
  1) **GP**	- Games played [integer]
  2) **MIN**	- Average minutes played per game [numeric]
  3) **PTS**	- Average points per game	[numeric]	
  4) **FGM**	- Average field goals made per game [numeric]		
  5) **FGA**	- Average field goal attempts	[numeric]	
  6) **FG%**	- Average field goal success percentage [numeric]		
  7) **3P Made** - Average three-pointers made per game [numeric]	
  8) **3PA**	- Average three-pointers attempted per game [numeric]		
  9) **3P%**	- Average three-pointer success percentage per game	[numeric]	
  10) **FTM**	- Average free throws made per game [numeric]		
  11) **FTA**	- Average free throws attempted per game	[numeric]	
  12) **FT%**	- Average free throw success percentage	[numeric]	
  13) **OREB**	- Average offensive rebounds per game [numeric]		
  14) **DREB**	- Average defensive rebounds per game [numeric]		
  15) **REB**	- Average rebounds per game [numeric]				
  16) **AST**	- Average assists per game [numeric]				
  17) **STL**	- Average steals per game [numeric]				
  18) **BLK**	- Average blocks per game [numeric]				
  19) **TOV**	- Average turnovers per game [numeric]				
  20) **Year_Drafted** - Year of draft [integer]		
  21) **Round_Drafted**	- Round of draft [character - contains 'U' for undrafted]		
  22) **Team** - Team played for [character]	
  23) **Win_Percentage** - Team's win percentage [numeric]
  24) **Position** - Position played in [character]		
  25) **Year_Debut** - Year of debut [integer]

## Preview raw data
```{r preview_raw}
sprintf("%s observations of %s variables", nrow(raw_data), ncol(raw_data))
kable(head(raw_data)) # %>%
#  kable_styling(bootstrap_options = c("striped", "bordered", "hover", "condensed", "responsive")) 
# # scroll_box(width = "800px")
#glimpse(raw_data)
#str(raw_data)
```

# II: Exploratory Data Analysis (EDA) of raw data

## Summary statistics 

Using the powerful "summary" function, we can check each column for missing, negative (where there shouldn't be), infinite and anomalous values. We can also make sure all values fall within obvious expected ranges for particular variables (year drafted, win percentage etc.), using the minimum and maximum statistics. 

```{r summary_statistics}
summary(raw_data)
# counts of outcome variable
#kable(raw_data %>% count(TARGET_5Yrs) %>% mutate(proportion = round(n/sum(n),2)))
```

**Insights:**

  1) Values for year drafted and year debut are all in the expected range (between 1971 and 2016)
  2) Values for win percentage are all between 0 and 100, as expected
  3) The character variables (name, round drafted, team and position) all have the expected number of observations (1301).
  4) Using the TARGET_5Yrs variable, we see that 818 (63%) of players in the sample reached 5 years, and 483 37% did not
  
5) A small number of NAs are present in the expected 3 pointers and win percentage columns (10 and 1 observations respectively). Given this is less than 1% of the sample size, we make the decision to drop all observations with a missing value for any variable, as it will make the model-fitting easier while costing very little in accuracy.
We drop the player names at this point, as they will serve no purpose in our analysis. We also drop the team names, as most of the predictive power of the team should be captured by the team win percentage variable (which is far easier to work with than 30 dummy variables for the teams):

```{r omit_nas}
data <- na.omit(raw_data) %>% select(-Name, -Team)
sprintf("NAs omitted, number of obs falls from %s to %s", nrow(raw_data), nrow(data))
```

## Check character variable values

Summarise does a good job of making sure the numeric values all fall within the expected ranges. However, non-numeric variables can also contain errors. We check for this by getting the distinct values for the two remaining columns with character type (Round Drafted and Position)

```{r check_character_variable_values}
data %>% distinct(Round_Drafted) %>% arrange(Round_Drafted)
data %>% distinct(Position) %>% arrange(Position)
```
We find a number of issues which need addressing

**Round drafted**
  - There are two different representations of U (undrafted), because one of them has a trailing space
  - Our dataset doesn't contain any players that were drafted in round 9. This is noted but not likely to cause issues, and is because a) the sample is quite small, and b) round 9 didn't exist past 1989. This will be discussed again later on.
  
**Position**
  - Again, there are two different representations of Center, due to a trailing space
  - There are two different representations of Point guard, this time due to a letter case
  
We now fix these issues:

```{r fix_character_variable_values}
# remove any leading or trailing whitespace in chracter columns
data <- data.frame(lapply(data, function(x) if(class(x)=="character") trimws(x) else(x)), stringsAsFactors=F)

# replace 'Point Guard' with 'Point guard'
data$Position[data$Position == "Point Guard"] <- "Point guard"

# check it worked
data %>% distinct(Round_Drafted) %>% arrange(Round_Drafted)
data %>% distinct(Position) %>% arrange(Position)
```

## Correlations between regressors and outcome variable

As a simple way to get a sense of which of the numeric regressors are most likely to be have predictive power, we take the correlation coefficient of each numeric regressor with the outcome variable (TARGET_5Yrs)

```{r corr_regressors_with_outcome}
correlations <- round(cor(as.numeric(data$TARGET_5Yrs), Filter(is.numeric, data)),2)
correlations[,order(abs(correlations),decreasing = TRUE)]
# kable(round(cor(data$TARGET_5Yrs, Filter(is.numeric, data)),2)) # round to 2 d.p. and pretty-print
# 
# test[,order(abs(test),decreasing = TRUE)]

```

While the correlation coefficient only measures linear relationships, we conclude that metrics involving three-pointers, and years of draft and debut, are likely to be poor predictors of success. Interestingly, so too is team win percentage - suggesting players cannot simply ride the coattails of their team to achieve individual success.

As we have a large number of predictors, this information may be used when deciding which predictors to include in certain models. 

<!-- ```{r corrs_between_regressors} -->
<!-- library(plotly) -->

<!-- corr_matrix = cor(Filter(is.numeric, raw_data)) -->
<!-- corr_matrix_diagonal = corr_matrix -->
<!-- corr_matrix_diagonal[lower.tri(corr_matrix_diagonal, diag = TRUE)] <- NA -->
<!-- corr_matrix_diagonal <- corr_matrix_diagonal[-1, -ncol(corr_matrix_diagonal)] -->
<!-- #corr_matrix = cor(raw_data %>%  select(TOV, BLK)) -->
<!-- corrplot(corr_matrix , type = "upper", tl.col = "black" , tl.srt = 45) -->

<!-- plot_ly(x = rownames(corr_matrix), y = colnames(corr_matrix), z = corr_matrix_diagonal, type = "heatmap") -->

<!-- ``` -->

<!-- ## Distributions of numeric variables, by outcome -->

<!-- ```{r, fig.height= 10, fig.width = 10, warning=FALSE} -->
<!-- numerics_long <- Filter(is.numeric, data) %>%  -->
<!--   pivot_longer(!TARGET_5Yrs, names_to = "variable", values_to = "value") -->
<!-- numerics_long$TARGET_5Yrs <- as.character(numerics_long$TARGET_5Yrs) -->

<!-- ggplot(numerics_long, aes(value, fill = TARGET_5Yrs)) + geom_boxplot() + facet_wrap(~variable, scales = "free") + labs(x = "") -->
<!-- # ggplot(numerics_long, aes(value, fill = TARGET_5Yrs)) + geom_histogram() + facet_wrap(~variable, scales = "free") + labs(x = "") -->
<!-- ``` -->

<!-- We see that for most variables, the distribution of values for players who went on to succeed is higher. This makes sense, as most of these variables are increasing in player quality (the better player is expected to make more blocks, steals etc.)  -->

## Distributions of character variables, by outcome

For the non-numeric variables, we can't use correlation coefficients to gauge predictive power. However, we can inspect the relationship between the variable and the outcome through bar charts.

```{r char_vars_bar_charts, warning=FALSE}
# raw_categorics_long <- raw_data %>%
#   select(TARGET_5Yrs, Round_Drafted, Team, Position) %>%
#   pivot_longer(!TARGET_5Yrs, names_to = "variable", values_to = "value") %>%
#   group_by(variable, value, TARGET_5Yrs) %>%
#   summarise(sum = n())
#
# raw_categorics_long$TARGET_5Yrs <- as.character(raw_categorics_long$TARGET_5Yrs)
#
# bar_chart <- ggplot(raw_categorics_long, aes(sum, fill = value)) + geom_bar() + facet_wrap(~variable, scales = "free")
#
# bar_chart

# bar_plot = function(variable){
#   data <- raw_data[,c("TARGET_5Yrs",variable)]
#   colnames(data)[2] = "variable"
#   data <- data %>%
#     group_by(TARGET_5Yrs, variable) %>%
#     tally() %>%
#     mutate(proportion = n/sum(n))
# 
#   data$TARGET_5Yrs <- as.character(data$TARGET_5Yrs)
# #  data$var <- as.character(data$var)
# 
#   print(data %>% ggplot(aes(x = variable , y = proportion, fill = TARGET_5Yrs))+
#     geom_bar(stat = "identity", width = 0.4, position = position_dodge(width = 0.5)))
# }
# 
# variables = c("Round_Drafted","Position")
# 
# for (i in variables){
#   bar_plot(i)
# }

# plot for round drafted
data %>%
  select(Round_Drafted, TARGET_5Yrs) %>%
  group_by(Round_Drafted, TARGET_5Yrs) %>%
  tally() %>%
  group_by(TARGET_5Yrs) %>%
  mutate(proportion = n/sum(n)) %>%
  ggplot(aes(x = Round_Drafted, y = proportion, fill = TARGET_5Yrs)) + geom_bar(stat = "identity", position = position_dodge()) + scale_x_discrete(limits = c(seq(1, 10), "U")) + ylab("proportion of players drafted in that round")

# plot for position
data %>%
  select(Position, TARGET_5Yrs) %>%
  group_by(Position, TARGET_5Yrs) %>%
  tally() %>%
  group_by(TARGET_5Yrs) %>%
  mutate(proportion = n/sum(n)) %>%
  ggplot(aes(x = Position, y = proportion, fill = TARGET_5Yrs)) + geom_bar(stat = "identity", position = position_dodge()) + scale_x_discrete(limits = c("Point guard", "Shooting guard", "Center", "Power forward", "Small forward")) + ylab("proportion of players drafted in that round")
```

**Round drafted**
In basketball, better players are generally drafted in an earlier round (with a lower round number), with the undrafted (U) players likely to be the worst. 

There is a clear difference in the distribution of round numbers for those who reached 5 years and those who did not. Among those with TARGET_5Yrs = 1, more than 65% were drafted in the first round, and fewer than 10% were undrafted. Among those who didn't reach 5 years, the corresponding values are 30% and 20%.

**Position**
To give the order of the bars some meaning, we order by position on the field (i.e. point guards are furthest back, and small forwards furthest forward).

We are then able to see that not only is there a slight difference in distribution of positions between those who made 5 years and those who didn't, but also the difference is successful players are slightly skewed towards attacking positions.

We conclude that both these variables are likely to be of value. 

# III: Pre-process dataset

## Create indicator variables

Using the "step_dummy" function from recipes (part of tidymodels), create dummy columns for the categoric variables (Round Drafted and Position)

```{r create_indicators, warning=FALSE}
# data_to_split <- data %>%
#   mutate(n = 1) %>%
#   pivot_wider(names_from = Round_Drafted, values_from = n,
#               names_prefix = 'round_drafted_', values_fill = list(n = 0)) %>%
#   mutate(n = 1) %>%
#   pivot_wider(names_from = Position, values_from = n,
#               names_prefix = 'position_', values_fill = list(n = 0))

dummy_multi_choice_rec <- recipe(~ ., data = data) %>%
  step_dummy(Round_Drafted) %>%
  step_dummy(Position) %>%
  prep()

data_to_split <- bake(dummy_multi_choice_rec, new_data = data) #%>% select(-Name)
#ids <- tidy(dummy_multi_choice_rec, number = 1)

kable(head(data_to_split))
# glimpse(data_to_split) #
```

## Split dataset into training and testing

Using the "initial_split" function from rsample (part of tidymodels), split data into training and testing, using (the default) 75% of observations for training and 25% for testing. The same data and samples will be used by all models. To preserve the overall class distribution, we stratify on the outcome variable (TARGET_5Yrs).

```{r train_test_split}
set.seed(1423)
data_split <- initial_split(data_to_split, strata = TARGET_5Yrs)

data_training <- data_split %>% training()
data_testing <- data_split %>% testing()

sprintf("Data had %s obs, now split into training (%s obs) and testing (%s obs)", nrow(data), nrow(data_training), nrow(data_testing))

kable(data_testing %>% count(TARGET_5Yrs) %>% mutate(proportion = round(n/sum(n),2)))
kable(data_training %>% count(TARGET_5Yrs) %>% mutate(proportion = round(n/sum(n),2)))

set.seed(1450)
## undummied
data_split_undummied <- initial_split(data, strata = TARGET_5Yrs)

data_training_undummied <- data_split_undummied %>% training()
data_testing_undummied <- data_split_undummied %>% testing()
```

# IV: Model development

## Logistic regression with only few predictors

#### Implementation of Gradient Descent

As a first step towards fitting a logistic regression model, we implement a gradient descent algorithm to find the model parameters that minimise the log loss function given by:

$$
L(\beta) = - \frac{1}{n}\sum_{i=1}^{n} [y_{i} \times log(\sigma(x_{i}^T \beta)) + (1-y_{i})\times log(1-\sigma(x_{i}^T \beta))]
$$

where:

* X is the matrix of  predictor variables
* y is the vector of the outcome variable
* $\beta$ is the vector of coefficients in the logistic regression model
* $\sigma$ is the logistic sigmoid function given by $\sigma(x_i^T\beta) = \frac{1}{1+e^-(x_i^T\beta)}$

For simplicity we omit the division by 1/n as it doesn't affect our minimisation problem.

**Note**: Here we assume an unregularised logistic regression with 0 penalty term

```{r}
# defining the sigmoid function
sigmoid <- function(x) {
  1 / (1 + exp(-x))
}
```
```{r}
# implementing the loss function in code
logistic_loss <- function(X, y, beta) {
  X <- unname(as.matrix(X)) # remove column names
  y <- as.numeric(as.character(y)) # converting the factors of the outcome first to strings then to numeric type
  loss <- 0
  for (i in 1:nrow(X)) {
    X_i_B <- sum(X[i, ] * beta)
    p_hat <- sigmoid(X_i_B)
    loss <- loss + (y[i] * log(p_hat) + (1 - y[i]) * log(1 - p_hat))
  }
  loss
}
```

The gradient of the log loss function is:

$$
\nabla L(\beta) = X^T(\sigma(X\beta)-Y)
$$

```{r}
# implementing the gradient in code
gradient <- function(X, y, beta) {
  X <- unname(as.matrix(X))
  y <- as.numeric(as.character(y))
  beta <- as.matrix(beta, length(beta), 1) # converting a beta to a column vector of length length(beta)
  as.matrix(t(X) %*% (sigmoid(X %*% beta) - y), ncol(X), 1) # returning the gradient vector as a column vector of length ncol(X)
}
```

```{r}
# the gradient descent algorithm
# source: https://en.wikipedia.org/wiki/Gradient_descent#Description
gradient_descent <- function(X, y, loss_function) {
  X <- unname(as.matrix(X))
  X <- scale(X) # standardising the predictors (columns in the X matrix) for greater stability
  y <- as.numeric(unlist(y))
  i <- 0
  beta_prev <- as.matrix(rep(0, ncol(X)), ncol(X), 1) # initialise the coefficient vector to be all zero
  initial_loss_value <- logistic_loss(X, y, beta_prev)
  gamma <- 0.01 # the step
  beta_next <- beta_prev - as.matrix((gamma * gradient(X, y, beta_prev)), ncol(X), 1)
  while (abs(loss_function(X, y, beta_next) - loss_function(X, y, beta_prev)) > 0.001) {
    beta_diff <- gradient(X, y, beta_next) - gradient(X, y, beta_prev)
    gamma_num <- as.numeric(t(beta_next - beta_prev) %*% beta_diff)
    gamma_denom <- sum(beta_diff^2)
    gamma <- abs(gamma_num) / gamma_denom # Update step
    beta_prev <- beta_next
    beta_next <- beta_prev - as.matrix((gamma * gradient(X, y, beta_prev)), ncol(X), 1)
    i <- i + 1
  }
  final_loss_value <- logistic_loss(X, y, beta_next)
  print(paste0("Initial loss: ", initial_loss_value))
  print(paste0("Final loss after ", i, " iterations: ", final_loss_value))
  beta_next # estimated coefficient values
}
```

We now apply the gradient-descent algorithm to estimate the coefficients of a logistic regression of our target on a subset of predictors.
The chosen predictors are likely to be weakly correlated, hence they avoid a potential multicollinearity problem in our model.
The chosen predictors (which are continuous variables) capture the general performance of a player in both offence and defense without going into more sophisticated detailes. The predictors are: GP, FTM, REB, AST, STL, BLK, TOV.

```{r}
# selecting the outcome and predictors from the train set
data_logistic <- data_training %>% select("GP", "PTS", "REB", "AST", "STL", "BLK", "TOV", "TARGET_5Yrs")
# scaling the variables
for (var in c("GP", "PTS", "REB", "AST", "STL", "BLK", "TOV")){
  data_logistic[ , var] <- scale(data_logistic[ , var])
}
X_gd <- data_logistic %>% select(-"TARGET_5Yrs")
y_gd <- data_training[, "TARGET_5Yrs"]
gradient_descent(X_gd, y_gd, logistic_loss)
```

To evaluate the efficiency of our algorithm, we compare the estimated coefficients from the gradient descent algorithm to those from the direct implementation of the logistic regression (using the glm library)

```{r}
logistic_fit <- glm(unlist(data_logistic[, "TARGET_5Yrs"]) ~ GP + PTS + REB + AST + STL + BLK + TOV - 1, data=data_logistic, family="binomial") #we omit the intercept
summary(logistic_fit)
```

As we can see, the estimated coefficients are quite similar between the two implementations, confirming that the gradient descent algorithm is a highly efficient optimisation algorithm.

### Feature selection with logistic regression

The model development section starts with a logistic regression model fit on all predictors. As an introductory stage, the model will also be used to evaluate significance of our predictor variables according to the z-tests in the summary below.

```{r}
#fit using tidy models
logistic_model <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification") %>%
  fit(TARGET_5Yrs ~., data=data_training)
tidy(logistic_model) %>% as_tibble() %>% print(n=40)
```

*Note*: The logistic model includes one dummy variable for all but one level (the reference level) of each categorical variable.

We can use the p-values shown in the output to conclude whether the variable coefficients are significant (at the 5% level). We see that the only significant coefficients are:

* GP : games played per season
* Year_Drafted : the year the player got drafted
* Win_Perc : the winning percentage of the team the player plays for
* Year_Debut : the year the player debuted his career

This is indeed a striking result: one might expect the game performance of a player to be indicative of how his talent and therefore of his ability to last longer in the NBA. With the debute and draft year variables being significant, there seems to be a time trend in the NBA drafting process whereby talented players tend to get drafted around the same time. A possible explanation is that drafting more talented players at the same time increases competitiveness in the league, pushing talented players to play even better and stay longer in the NBA.

However, we should note that all the variables related to points scored by a player in a game such as the total number of points, free throws, 3 pointers etc. are all highly correlated. In particular, this may mean that the insignificance of the corresponding coefficients is a result of multicollinearity and not the true lack of an association with the outcome variable.
Similarly, it may be argued that performance on offense and defense captured by variables such as steal, block, assist, rebound etc. exhibit correlation. Top players in the NBA are generally good in both offense and defense.

A final note on significance regarding categorical variables:
Logistic regression uses *reference level coding*. The z-test comapres each of the non-reference categories to the reference (ommitted) one. It is quite possible for none of those to be significant, but there to be significant differences amongst the non-reference categories.

**Predictions and model accuracy**

Test data prediction
```{r}
logistic_pred_results <- logistic_model %>%
  predict(data_testing) %>%
  bind_cols(data_testing) %>%
  glimpse()
logistic_pred_probs <- logistic_model %>%
  predict(data_testing, type="prob") %>%
  bind_cols(data_testing)
```

Model Evaluation

```{r}
#accuracy
logistic_pred_results$TARGET_5Yrs <- as.factor(logistic_pred_results$TARGET_5Yrs)
##truth refers to the actual results and estimate refers to what the model predicted
metrics(logistic_pred_results, truth=TARGET_5Yrs, estimate = .pred_class)
```

The logistic regression model yields an accuracy of nearly 78%. For our classification task, this can be seen as a good score indicating that a linear classification boundary fits our data well.

As a next step, we check for non-linearity using another classification approach - that of Support Vector Machines (SVM). As opposed to logistic regressional, SVM allow us to classify data points according to non-linear decision boundaries.

### SVM: a non-linear classifier.

#### SVM with a radial kernel

When using a Radial Basis (RBF) Kernel, the SVM algorithm measures the Eucledian distance (as a measure of similarity) between points in the space. Consequently, it has a very local behaviour in the sense that only nearby training observations affect the classification of a test observation.
Similar to the KNN algorithm, RBF Kernel has the advantage of overcoming space complexity as, instead of reusing all of the dataset, it only needs to store the support vectors during training. The support vectors are the data points that are the closest to the decision boundary and define the margin of the classifier

```{r}
svm_rbf <- svm_rbf() %>%
  set_engine("kernlab") %>%
  set_mode("classification") %>%
  fit(TARGET_5Yrs ~ ., data = data_training)
svm_rbf
```

**Predictions and model accuracy**

```{r}
predictions_svm <- svm_rbf %>%
  predict(data_testing) %>%
  bind_cols(data_testing)
predictions_svm$TARGET_5Yrs <- as.factor(predictions_svm$TARGET_5Yrs)
metrics(predictions_svm, truth=TARGET_5Yrs, estimate = .pred_class)
```

The RBF Kernel yields a predictive accuracy of about 75% which is slightly lower than logistic regression.

#### SVM with a polynomial kernel

```{r}
svm_poly <- svm_poly() %>%
  set_engine("kernlab") %>%
  set_mode("classification") %>%
  fit(TARGET_5Yrs ~., data=data_training)
svm_poly
```


**Predictions and model accuracy**

```{r}
predictions_svm_poly <- svm_poly %>%
  predict(data_testing) %>%
  bind_cols(data_testing)
predictions_svm_poly$TARGET_5Yrs <- as.factor(predictions_svm_poly$TARGET_5Yrs)
metrics(predictions_svm_poly, truth=TARGET_5Yrs, estimate = .pred_class)
```

The polynomial kernel yields an accuracy of 76% which is lower but closer to the accuracy of logistic regression.

From these two results, we can surprisingly conclude that a linear classification model has the highest accuracy on our data set.

## Decision Tree and Random Forest

We now move onto a very different classification approach: decision-tree classification. Decision trees have the advantage of being very interpretable, allowing us to directly visualise the relationship between the outcome and the explanatory variables.

To get more sensible results, we use the undummied version of our data.

```{r}
#data_training$TARGET_5Yrs <- as.factor(Train$TARGET_5Yrs)
tree <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification") %>%
  fit(TARGET_5Yrs ~ ., data = data_training)

tree %>%
  extract_fit_engine() %>%
  rpart.plot(roundint=FALSE, type = 5)

##Predictions and model accuracy

predictions_tree <- tree %>%
  predict(data_testing) %>%
  bind_cols(data_testing)
predictions_tree$TARGET_5Yrs <- as.factor(predictions_tree$TARGET_5Yrs)
metrics(predictions_tree, truth=TARGET_5Yrs, estimate = .pred_class)
```

**Interpretation of results**

 * Year of debut seems to be the most important factor determining the success of an NBA player.
  * Time as a factor: Regardless of other factors, it seems that players who debuted after 2013 do not have long careers in the NBA. This may be due to the fact that the 2011-2012 season was shortened from the normal 82 games per team to 66, after a new collective bargaining agreement (CBA) took place between owners of NBA teams and the players themselves, causing nearly two months of inactivity before it has been finally reached. Although such lockouts are not common in the NBA league, capital loss and concurrency between small and big market teams can cause imbalance within the NBA Association. Such lockouts are expected to decrease the performance of players as well as the potential of talented rookies getting into the league in the course of the season.
 * The round the player gets drafted in is a second important factor. It seems that players who do not get drafted in the 1st round are almost guaranteed to last long in the NBA (provided they play more than 50 games per season). Although this result may seem surprising, it is usually the case that team owners prefer drafting players in susbequent rounds rather than in the first, given how much expectations are put in the first rounders and how much it is expensive to hire them.

* This tree-like visualisation is very useful in that it allows one with very little background understanding to generate predictions regarding the classification of an NBA player, by moving through the tree from top to bottom.
  * Every node consists of 3 numbers: the top one denotes the predicted class (0 or 1), the middle one denotes the problability of belonging to class 1, and last one denotes the percentage of the observations used in that node.

**Observations**

* The tree classifier yields an overall predictive accuracy of 74%.
* The relatively low predictive accuracy is compensated by the simplicity and the interpretability of the decision tree model. Since there are several other factors affecting a player's success in the NBA, some of which we can't control for in our models (such as talent, socio-economic background etc.), it is not surprising that our decision tree does not achieve an exceptionally high predictive accuracy. For what it is, the model seems to be performing relatively well.

### Random Forests

To improve upon the performance of the decision tree classifier, we consider a random forest model. Random forests use decision trees as building blocks to construct more powerful predictive models.

Decision trees have low bias (they make no assumption about the functional form of the true relationship between the outcome and the predictors) but suffer from high variance (in that the structure of the decision tree is highly sensitive to changes in individual points of the data, so the model certainly overfits). Random forests attempt to address that issue by lowering variance at the expense of a small increase in bias.

Random forest is built by fitting multiple decision trees, then taking the average of the resulting predictions. Each time a split is considered, a random sample of m predictors is chosen as split candidates from the full set of preditors. This ensures that the resulting predictions of the decision trees are not highly correlated (averaging over highly correlated values does not lead to substantial reduction in variance).

As for tree, we also refer to the undumied version of our data
```{r}
#random forests fit with tidymodels
rf <- rand_forest(trees=100, mode="classification") %>%
    set_engine("randomForest") %>%
    fit(TARGET_5Yrs~., data = data_training)
rf
```

**Predictions and model accuracy**
```{r}
#predictions
predictions_rf <- rf %>%
  predict(data_testing) %>%
  bind_cols(data_testing) %>%
  glimpse()
```

```{r}
#using metrics to measure the performance of the model
predictions_rf$TARGET_5Yrs <- as.factor(predictions_rf$TARGET_5Yrs)
##truth refers to the actual results and estimate refers to what the model predicted
metrics(predictions_rf, truth=TARGET_5Yrs, estimate = .pred_class)
```

Random forests achieve a predictive accuracy of roughly 78% which is higher than our previous single decision tree classifier, as expected.
```{r}
#obtaining the probabilities for each possible predicted value
predictions_probs <- rf %>%
  predict(data_testing, type="prob") %>%
  bind_cols(data_testing)
glimpse(predictions_probs)
```

```{r}
#plotting the results
predictions_probs %>%
  roc_curve(as.factor(TARGET_5Yrs), .pred_0) %>%
  autoplot()
```

The Receiver Operating Characteristic (ROC) curve plots sensitivity (true positive rate TPR) against 1-specificity (false positive rate FPR). It is known to be a good measure of the performance of a binary classifier.
 * *Specificity*: measure of a model's ability to predict true negatives. It measures the proportion of observations that were correctly predicted to be positive out of all negative observations.
 * *Sensitvity*: measure of a model's ability to predict true positives. It measures the proportion of observations that were correctly predicted to be positive out of all positive observations.

*Note*: The ROC measure is useful as it does not depend on the class distribution.

 Our random forest classifier yields a ROC curve that is significantly above and far of the baseline 45 degrees line where FPR = TPR. This means our classifier gives fairly accurate results in accordance with its relatively high predictive accuracy.


## Lasso
  - Relatively interpretable 
  - High Dimensionality 

The first step is to take all of the appropriate variables from our dataset and separate the regressors and our outcome variable. 

```{r lasso_variable_selection}
# Train_data_KNN <- data_training %>%
#   select(X3P.,Win_Perc,GP,MIN,PTS,FG.,FT.,OREB,DREB,AST,STL,BLK,TOV,Year_Drafted, starts_with("Round_Drafted"),starts_with("Position"))

# Test_data_KNN <- data_testing %>%
#   select(X3P.,Win_Perc,GP,MIN,PTS,FG.,FT.,OREB,DREB,AST,STL,BLK,TOV,Year_Drafted, starts_with("Round_Drafted"),starts_with("Position"))

data_training_lasso <- data_training %>%
  select(-FGA, -X3PA, -FTA, -REB, -Year_Drafted, -TARGET_5Yrs)

data_testing_lasso <- data_testing %>%
  select(-FGA, -X3PA, -FTA, -REB, -Year_Drafted, -TARGET_5Yrs)

outcome_training <- data_training %>% select(TARGET_5Yrs)
outcome_testing <- data_testing %>% select(TARGET_5Yrs)
```


With 34 different regressors, this may not be the optimal set of regressors for our model. We want to complete some process of subset selection to not only draw some inference about what variables may be more significant, but also reduce the dimensionality of the KNN model as KNN is susceptible to the curse of dimensionality. 

Additionally, in 1989 the structure of the draft changed from 10 rounds to just 2. In order to avoid having to remove data values, but still accurately represent the significance of being drafted, we will need to create a time dummy variable and add interaction terms to the Round_Drafted_# dummy variables. 


```{r round_drafted_year_debut_interactions}
# Create Dummy variables for debut before 1989
data_training_lasso$debut_before_1989 <- ifelse(data_training_lasso$Year_Debut >= 1989, 0, 1)
data_testing_lasso$debut_before_1989 <- ifelse(data_testing_lasso$Year_Debut >= 1989, 0, 1)

# Create Interaction terms for Round_Drafted_# with Year_debut_before_1989
names <- paste0("Round_Drafted_", c("1","10","2","3","4","5","6","7","8","U"), "_before_1989")

data_training_lasso[names] <- lapply(select(data_training_lasso,starts_with("Round_Drafted")), function(x) x*data_training_lasso$debut_before_1989)

data_testing_lasso[names] <- lapply(select(data_testing_lasso,starts_with("Round_Drafted")), function(x) x*data_testing_lasso$debut_before_1989)


# # Create Interaction terms for Round_Drafted_# and Year_debut_before_1989
# Train_data_KNN$Round_Drafted_1_before1989 <- Train_data_KNN$Round_Drafted_X1*Train_data_KNN$Year_debut_before_1989
# Train_data_KNN$Round_Drafted_2_before1989 <- Train_data_KNN$Round_Drafted_X2*Train_data_KNN$Year_debut_before_1989
# Train_data_KNN$Round_Drafted_3_before1989 <- Train_data_KNN$Round_Drafted_X3*Train_data_KNN$Year_debut_before_1989
# Train_data_KNN$Round_Drafted_4_before1989 <- Train_data_KNN$Round_Drafted_X4*Train_data_KNN$Year_debut_before_1989
# Train_data_KNN$Round_Drafted_5_before1989 <- Train_data_KNN$Round_Drafted_X5*Train_data_KNN$Year_debut_before_1989
# Train_data_KNN$Round_Drafted_6_before1989 <- Train_data_KNN$Round_Drafted_X6*Train_data_KNN$Year_debut_before_1989
# Train_data_KNN$Round_Drafted_7_before1989 <- Train_data_KNN$Round_Drafted_X7*Train_data_KNN$Year_debut_before_1989
# Train_data_KNN$Round_Drafted_8_before1989 <- Train_data_KNN$Round_Drafted_X8*Train_data_KNN$Year_debut_before_1989
# Train_data_KNN$Round_Drafted_10_before1989 <- Train_data_KNN$Round_Drafted_X10*Train_data_KNN$Year_debut_before_1989
# Train_data_KNN$Round_Drafted_Undrafted_before1989 <- Train_data_KNN$Round_Drafted_U*Train_data_KNN$Year_debut_before_1989
# Test_data_KNN$Round_Drafted_1_before1989 <- Test_data_KNN$Round_Drafted_X1*Test_data_KNN$Year_debut_before_1989
# Test_data_KNN$Round_Drafted_2_before1989 <- Test_data_KNN$Round_Drafted_X2*Test_data_KNN$Year_debut_before_1989
# Test_data_KNN$Round_Drafted_3_before1989 <- Test_data_KNN$Round_Drafted_X3*Test_data_KNN$Year_debut_before_1989
# Test_data_KNN$Round_Drafted_4_before1989 <- Test_data_KNN$Round_Drafted_X4*Test_data_KNN$Year_debut_before_1989
# Test_data_KNN$Round_Drafted_5_before1989 <- Test_data_KNN$Round_Drafted_X5*Test_data_KNN$Year_debut_before_1989
# Test_data_KNN$Round_Drafted_6_before1989 <- Test_data_KNN$Round_Drafted_X6*Test_data_KNN$Year_debut_before_1989
# Test_data_KNN$Round_Drafted_7_before1989 <- Test_data_KNN$Round_Drafted_X7*Test_data_KNN$Year_debut_before_1989
# Test_data_KNN$Round_Drafted_8_before1989 <- Test_data_KNN$Round_Drafted_X8*Test_data_KNN$Year_debut_before_1989
# Test_data_KNN$Round_Drafted_10_before1989 <- Test_data_KNN$Round_Drafted_X10*Test_data_KNN$Year_debut_before_1989
# Test_data_KNN$Round_Drafted_Undrafted_before1989 <- Test_data_KNN$Round_Drafted_U*Test_data_KNN$Year_debut_before_1989
```

Below we are going to estimate the Lasso Regression model using the GLMNET function. This entails setting alpha = 1. It should be noted that by changing the family parameter to "binomial",the GLMNET function "fits a traditional logistic regression model for the log-odds" according to the documentation. For Lambda, we are using a sequence from -6 to 0 in increments of 0.5. 

```{r lasso_model}
#Esimate the Lasso Regression Model 
model_lasso <- glmnet(data.matrix(data_training_lasso), y = data.matrix(outcome_training), intercept = FALSE, alpha = 1 , lambda = 10^seq(from = -6, to = 0, by = 0.5), family = "binomial")
#Plot the Lasso Regression Model against Log Lambda Values
plot(model_lasso, xvar = "lambda")

beta_hat <- coef(model_lasso)
#Return the coefficients of the Lasso Regression Model for the different values of Lambda 
beta_hat

## Find the optimal value of Lambda

# The code below estimates the value for Lambda that gives the largest area beneath the ROC (Receiver Operating Characteristic) Curve. These areas are esimated using K-fold cross Validation. 
cv_lasso <- cv.glmnet(x = data.matrix(data_training_lasso), y = data.matrix(outcome_training), type.measure = "auc" , nfolds = 10)
lambda = cv_lasso$lambda.min
#Return the coefficients associated with the model that produces the largest area under the ROC curve in the training data. 
beta_hat2 <- coef(model_lasso, s = lambda)
plot(cv_lasso)
print(beta_hat2)
```

### Analysis of the Lasso model above 


Printing the coefficient estimates for a range of Lambdas was insightful as it was interesting to see what variables persist as the penality increases. 

Most Notably: Win_Perc (Win Percentage of Team in Debut Season),GP (Number of Games Played in Debut Season), MIN (Average Number of Minutes played per game), PTS (Average Number of Points per Game), FG. (Field Goal Percentage) , OREB (Average Offensive Rebounds per Game), AST (Average Number of Assists per Game), Year_Drafted (Year Drafted), Round_Drafted_1 (Dummy Variable for Players Drafted in Round 1)


One could develop a convincing story to justify the coefficients displayed above: 

Teams that win more games with a particular group will be more likely to keep most of those players, espcially the main contributers. Therefore, we should not be surprised that the coefficients for Win Percentage and Number of Games Played are positive and relatively large. 

What perhaps is most interesting is the fact that Defensive Rebounds, Steals and Turnovers seem to have a relatively insignificant impact on a player's ability to reach year 5. Whilst there are players who pride themselves on their ability to defend (E.g. Draymond Green), in reality the structure of the game lends itself far more to incentivise scoring more points as opposed to defending more points. 


Important to note that we cannot consider this regression causal. Our dataset has not been collected randomly and we cannot consider our dataset as good as randomly selected as there are still cofounding variables that have not been accounted for. 

For example, we do not know if players may have been played through injuries or been injured in their final college season. This lack of information muddies any interpretation we can make on the data above. 

-A good example of this is a player not in our dataset Markelle Fultz. Markelle was drafted first overall in the 2017 NBA draft but had a terrible season after developing a neurogenic syndrome that inhibited his shooting motion. Despite this, Markelle played the entire season and put up terrible numbers. 
  
Our current specificaiton does not accurately capture circumstances where confounding variables like injury/ health, team chemistry / personality fit or off-the-field status can impact the likelihood of a player reaching year 5. 
  

When considering the effect of Round Drafted:

\begin{equation}

\beta_1Round\_Drafted\_\# + \beta_2Round\_Drafted\_\# \  * Year\_Debuted\_Before\_1989 \\

(\beta_1 \ +   \beta_2Year\_Debuted\_Before\_1989 )Round\_Drafted\_\# 

\end{equation}

Where:  

\begin{equation}   
Year\_Debuted\_Before\_1989 = 
     \begin{cases}
       1 &\quad\text{if Year_Debut}<1989\\
       0 &\quad\text{if Year_Debut} \ge1989 \\
     \end{cases}
\end{equation}     

In other words, the effect for players drafted after 1989 is captured by 
\begin{equation} \beta_1 \end{equation} 
and players drafted before 1989 is captured by 
\begin{equation} \beta_1 + \beta_2 \end{equation}
 
 
The data suggests a clear association between being drafted in the first round and the likelihood of making it to year 5. This is consistent with the notion that players drafted in the first round will have performed better in college or international leagues before they declared for the draft. You could argue that the round a player is drafted functions as a good proxy for the expectations for a player coming into the league. 


 - Compare this model with baseline models:
    * Logistic Regression
    * Trees 
    
  


## KNN

```{r tidymodels_knn}
data_training_KNN <- data_training_lasso %>%
  select(GP, MIN, FG., FTM, OREB, Win_Perc, Year_Debut, Round_Drafted_X1, Round_Drafted_1_before_1989) %>% 
  bind_cols(outcome_training)

data_testing_KNN <- data_testing_lasso %>%
  select(GP, MIN, FG., FTM, OREB, Win_Perc, Year_Debut, Round_Drafted_X1, Round_Drafted_1_before_1989) %>% 
  bind_cols(outcome_testing)

knn <- nearest_neighbor(mode = "classification", engine = "kknn") %>% 
  fit(TARGET_5Yrs ~., data = data_training_KNN)

predictions <- knn %>%
  predict(data_testing_KNN) %>%
  bind_cols(data_testing_KNN)
predictions$TARGET_5Yrs <- as.factor(predictions$TARGET_5Yrs)
metrics(predictions, truth=TARGET_5Yrs, estimate = .pred_class)
```

  - Predictive Accuracy without interpretability 


Using the results from the Lasso Regression, we want choose the following subset of regressors to train our KNN model: Win_Perc,GP, MIN, PTS, FG., OREB, AST, Year_Drafted, Round_Drafted_1

Given that KNN models are susceptible to the curse of dimensionality, it is important to choose the smallest number of variables as possible whilst trying to gain the most information from each variable to improve the predictive accuracy of the model. 

Looking at the Lasso results above, this subset of regressors seems to be represent that variables that capture the most information regardless of the magnitude of the penalty and appear to be the most significant. 


```{r KNN_variable_selection}
Adj_Train_data_KNN <- data_training %>% select("Win_Perc","GP", "MIN", "PTS", "FG.", "OREB", "AST", "Year_Drafted", "Round_Drafted_X1")
Adj_Test_data_KNN <- data_testing %>% select("Win_Perc","GP", "MIN", "PTS", "FG.", "OREB", "AST", "Year_Drafted", "Round_Drafted_X1")
```

```{r}
KNN_Predicted <- knn(train = data.matrix(Adj_Train_data_KNN), test = data.matrix(Adj_Test_data_KNN), cl = data.matrix(outcome_training), k = 25)
CrossTable(x= data.matrix(outcome_testing), y= KNN_Predicted, prop.chisq = FALSE, chisq = TRUE)
```

```{r}
(62+176)/324*100
```

accuracy on test data = 73.45679%

# V: Conclusion